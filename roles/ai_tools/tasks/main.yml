---
# Task to install vLLM via pip is REMOVED.

- name: Download and run Ollama installation script
  ansible.builtin.shell: curl -fsSL https://ollama.com/install.sh | sh
  args:
    creates: /usr/local/bin/ollama # Makes the task idempotent (don't run if binary exists)
  become: yes # The script itself likely requires root privileges

- name: Check if Ollama systemd service file exists
  ansible.builtin.stat:
    path: /etc/systemd/system/ollama.service
  register: ollama_service_stat

- name: Configure Ollama service state
  ansible.builtin.systemd:
    name: ollama
    enabled: "{{ start_services | default(false) }}"
    state: "{{ 'started' if start_services | default(false) else 'stopped' }}"
  become: yes
  when: ollama_service_stat.stat.exists

- name: Pull Infinity Embedding Server Docker image
  community.docker.docker_image:
    name: "{{ infinity_image }}"
    source: pull
  become: yes # Requires docker group membership or sudo

- name: Create specific directory on HOST for the Infinity model
  ansible.builtin.file:
    path: "{{ infinity_model_path_host }}" # e.g., /opt/models/all-MiniLM-L6-v2
    state: directory
    mode: '0755'
    owner: root # Adjust ownership/permissions if needed for non-root container user
    group: root
  become: yes

# Use separate tasks for Infinity GPU vs CPU based on toolkit configuration flag
- name: Run Infinity Embedding Server container (GPU Mode)
  community.docker.docker_container:
    name: infinity_server
    image: "{{ infinity_image }}"
    state: "{{ 'started' if start_services | default(false) else 'present' }}"
    restart_policy: unless-stopped
    ports:
      - "{{ infinity_port }}:7997" # Map host port to container port
    volumes:
      # Mount the specific model directory from HOST into the CONTAINER
      - "{{ infinity_model_path_host }}:{{ infinity_model_path_container }}:ro" # Mount read-only
    # gpus: all # Enable GPU access - requires nvidia-container-toolkit & configured Docker
    command: >
      --model-name-or-path {{ infinity_model_path_container }}
      --port 7997
      --host 0.0.0.0
  become: yes
  when: configure_nvidia_toolkit | default(false) # Run only if toolkit/GPU config is enabled

- name: Run Infinity Embedding Server container (CPU Mode)
  community.docker.docker_container:
    name: infinity_server
    image: "{{ infinity_image }}" # Ensure this image tag works on CPU
    state: "{{ 'started' if start_services | default(false) else 'present' }}"
    restart_policy: unless-stopped
    ports:
      - "{{ infinity_port }}:7997"
    volumes:
      - "{{ infinity_model_path_host }}:{{ infinity_model_path_container }}:ro" # Mount read-only
    # NO 'gpus: all' here
    command: >
      --model-name-or-path {{ infinity_model_path_container }}
      --port 7997
      --host 0.0.0.0
  become: yes
  when: not (configure_nvidia_toolkit | default(false)) # Run only if toolkit/GPU config is disabled


# --- vLLM Docker Container Setup ---

- name: Create Hugging Face cache directory
  ansible.builtin.file:
    path: "{{ models_base_dir }}/.cache/huggingface"
    state: directory
    mode: '0755'
    owner: root
    group: root
  become: yes

- name: Pull vLLM OpenAI Docker image
  community.docker.docker_image:
    name: "{{ vllm_docker_image }}"
    source: pull
  become: yes

- name: Run vLLM OpenAI container (GPU Mode)
  community.docker.docker_container:
    name: vllm_openai_server
    image: "{{ vllm_docker_image }}"
    state: "{{ 'started' if start_services | default(false) else 'present' }}"
    restart_policy: unless-stopped
    ports:
      - "{{ vllm_port }}:8000"
    volumes:
      - "{{ models_base_dir }}/.cache/huggingface:/root/.cache/huggingface"
    # gpus: all
    env:
      HUGGING_FACE_HUB_TOKEN: "{{ lookup('env', 'HF_TOKEN') | default('', true) }}"
      NVIDIA_VISIBLE_DEVICES: all
      VLLM_ATTENTION_BACKEND: "FLASHINFER"
    command: >
      --host 0.0.0.0
      --port 8000
      --model {{ vllm_model }}
      {% if vllm_tensor_parallel_size is defined %}--tensor-parallel-size {{ vllm_tensor_parallel_size }}{% endif %}
      {% if vllm_gpu_memory_utilization is defined %}--gpu-memory-utilization {{ vllm_gpu_memory_utilization }}{% endif %}
      {% if vllm_max_model_len is defined %}--max-model-len {{ vllm_max_model_len }}{% endif %}
      --dtype auto
      --use-v2-block-manager
      --kv-cache-dtype {{ vllm_kv_cache_dtype | default('auto') }}
      {% if vllm_calculate_kv_scales | default(false) %}--calculate-kv-scales{% endif %}
      --load-format {{ vllm_load_format | default('auto') }}
      {% if vllm_quantization is defined %}--quantization {{ vllm_quantization }}{% endif %}
  become: yes
  when: configure_nvidia_toolkit | default(false)

- name: "Run vLLM OpenAI container (CPU Mode - Warning: Likely very slow!)"
  community.docker.docker_container:
    name: vllm_openai_server
    image: "{{ vllm_docker_image }}" # Ensure image supports CPU mode
    state: "{{ 'started' if start_services | default(false) else 'present' }}"
    restart_policy: unless-stopped
    ports:
      - "{{ vllm_port }}:8000"
    volumes:
      - "{{ vllm_model_path_host }}:{{ vllm_model_path_container }}:ro"
    # NO 'gpus: all' here
    command: >
      --host 0.0.0.0
      --port 8000
      --model {{ vllm_model_path_container }}
      # Remove GPU specific args if they cause errors in CPU mode
      # {% if vllm_tensor_parallel_size is defined and vllm_tensor_parallel_size == 1 %} # TP size > 1 needs GPU
      # {% endif %}
      # {% if vllm_gpu_memory_utilization is defined %} # GPU memory util meaningless
      # {% endif %}
      {% if vllm_max_model_len is defined %}--max-model-len {{ vllm_max_model_len }}{% endif %}
      --dtype auto
      --device cpu
  become: yes
  when: not (configure_nvidia_toolkit | default(false))
