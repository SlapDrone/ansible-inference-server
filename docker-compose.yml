# for fp8 kv cache: by setting environment variable  VLLM_ATTENTION_BACKEND=FLASHINFER
# also: need to figure out vllm cli equivalent of calculate_kv_scales=True
# ───── anchors ─────
x-vllm-env: &vllm-env                    # map-style so we can extend it
  HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
  NVIDIA_VISIBLE_DEVICES: all
  VLLM_ATTENTION_BACKEND: FLASHINFER

x-vllm-base: &vllm-base
  image: vllm/vllm-openai:latest
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  ipc: host
  ports: ["8000"]
  environment: *vllm-env
  volumes:
    - ~/.cache/huggingface:/root/.cache/huggingface
  command: >-
    --host=0.0.0.0
    --port=8000
    --max-model-len=${MAX_MODEL_LEN}
    --gpu-memory-utilization=${GPU_MEM_UTIL}
    --use-v2-block-manager
    --kv-cache-dtype=${KV_CACHE_DTYPE}
    --calculate-kv-scales

# ───── services ─────
services:
  # classic HF checkpoint hosting
  vllm-standard:
    <<: *vllm-base
    container_name: vllm-standard
    ports:
      - "8000:8000"
    environment:
      <<: *vllm-env
      MODEL: ${HF_MODEL_ID}                # extra var only for this service
    # --load-format=${LOAD_FORMAT}
    command: >-
      --model=${HF_MODEL_ID}
      --quantization=${QUANTIZATION}

  # single-file GGUF hosting
  vllm-gguf:
    <<: *vllm-base
    container_name: vllm-gguf
    build: ./gguf-wrapper                 # custom entrypoint layer
    ports:
      - "8001:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models                  # place to store the .gguf
    environment:
      <<: *vllm-env
      GGUF_REPO: ${GGUF_REPO}
      GGUF_FILE: ${GGUF_FILE}
      TOKENIZER_REPO: ${TOKENIZER_REPO}
    entrypoint: ["/prefetch-and-run.sh"]
    command: >-
      --model=/models/${GGUF_FILE}
      --tokenizer=${TOKENIZER_REPO}
      --load-format=gguf

  infinity:
    image: michaelf34/infinity:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      # - INFINITY_MODEL_ID=BAAI/bge-large-en-v1.5
      - INFINITY_MODEL_ID=infgrad/jasper_en_vision_language_v1
      - INFINITY_PORT=8080
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8080:8080"
    command: >
      v2 --model-id ${INFINITY_MODEL_ID} --port ${INFINITY_PORT}
