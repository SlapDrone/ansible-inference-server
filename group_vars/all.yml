---
# --- General Settings ---
ansible_python_interpreter: /usr/bin/python3 # Adjust if python3 is elsewhere

# --- NVIDIA Settings ---
# Set to false since drivers are assumed to be pre-installed.
# Set to true ONLY if you want Ansible to ATTEMPT driver installation (e.g. 'ubuntu-drivers autoinstall').
install_nvidia_drivers: false

# Set to true to ensure NVIDIA Container Toolkit is installed and Docker is configured for GPU usage.
# This is needed even if drivers are pre-installed. Set to false only if you've manually done this too.
configure_nvidia_toolkit: true

# --- Model Settings ---
# Base directory where models will be downloaded and stored on the HOST machine
models_base_dir: /opt/models

# List of models to download from Hugging Face
# Add or remove models as needed. Specify 'repo_id' and optionally 'revision'.
models_to_download:
  - repo_id: "mistralai/Mistral-7B-Instruct-v0.1"
    # revision: "main" # Optional: specify a branch, tag, or commit hash
  - repo_id: "TheBloke/Llama-2-7B-Chat-GGUF" # Example GGUF for Ollama (Ollama can also download itself)
  - repo_id: "sentence-transformers/all-MiniLM-L6-v2" # Example for Infinity Embeddings

# --- vLLM Settings (Docker Container) ---
vllm_docker_image: "vllm/vllm-openai:latest" # Official image. Pin version (e.g., :v0.4.0.post1) for stability
vllm_port: 8000 # Port EXPOSED on the HOST
# REPO_ID of the model vLLM should serve initially (must be in 'models_to_download')
vllm_active_model_repo_id: "mistralai/Mistral-7B-Instruct-v0.1"
# Automatically derive the HOST path where the model is downloaded
vllm_model_path_host: "{{ models_base_dir }}/{{ vllm_active_model_repo_id | regex_replace('^.*/', '') }}"
# Define the path INSIDE the container where the model will be mounted
vllm_model_path_container: "/models/{{ vllm_active_model_repo_id | regex_replace('^.*/', '') }}"
# Optional vLLM runtime arguments (passed to the container command)
# vllm_tensor_parallel_size: 1 # Number of GPUs for tensor parallelism
# vllm_gpu_memory_utilization: 0.90 # Fraction of GPU memory to use
# vllm_max_model_len: 4096 # Max sequence length

# --- Infinity Settings ---
# Check Docker Hub for appropriate image tags (e.g., specific cuda versions)
infinity_image: "michaelfeil/infinity:latest"
infinity_port: 7997 # Port EXPOSED on the HOST
# REPO_ID of the embedding model Infinity should serve (must be in 'models_to_download')
infinity_active_model_repo_id: "sentence-transformers/all-MiniLM-L6-v2"
# Automatically derive the HOST path where the model is downloaded
infinity_model_path_host: "{{ models_base_dir }}/{{ infinity_active_model_repo_id | regex_replace('^.*/', '') }}"
# Define the path INSIDE the container where the model will be mounted
infinity_model_path_container: "/models/{{ infinity_active_model_repo_id | regex_replace('^.*/', '') }}"


# --- Ollama Settings ---
# Ollama usually runs on port 11434 and manages its own models/service after installation.

# --- Docker Settings ---
# Docker Compose V2 (plugin) is usually installed via apt now.
