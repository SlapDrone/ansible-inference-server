---
# --- General Settings ---
ansible_python_interpreter: /usr/bin/python3 # Adjust if python3 is elsewhere

# --- NVIDIA Settings ---
# Tell Ansible to call sudo with “-n” (never prompt)
ansible_become_flags: "-n"
# Set to false since drivers are assumed to be pre-installed.
# Set to true ONLY if you want Ansible to ATTEMPT driver installation (e.g. 'ubuntu-drivers autoinstall').
install_nvidia_drivers: false

# Set to true to ensure NVIDIA Container Toolkit is installed and Docker is configured for GPU usage.
# This is needed even if drivers are pre-installed. Set to false only if you've manually done this too.
configure_nvidia_toolkit: true

# --- Model Settings ---
# Base directory where models will be downloaded and stored on the HOST machine
models_base_dir: /opt/models

# List of models to download from Hugging Face
# Add or remove models as needed. Specify 'repo_id' and optionally 'revision'.
models_to_download:
  - repo_id: "mistralai/Mistral-7B-Instruct-v0.1"
  - repo_id: "Qwen/Qwen2.5-72B-Instruct-AWQ"
  # vanilla
  # - repo_id: "THUDM/GLM-4-32B-0414"
  # - repo_id: "THUDM/GLM-Z1-32B-0414"
  # - repo_id: "deepcogito/cogito-v1-preview-qwen-32B"
  # - repo_id: "featherless-ai/Qwerky-72B"
  # - repo_id: "featherless-ai/Qwerky-QwQ-32B"
  # - repo_id: "Qwen/Qwen2.5-32B-Instruct-GGUF"
  # - repo_id: "Qwen/Qwen2.5-72B-Instruct-GGUF"
  # - repo_id: "meta-llama/Llama-4-Scout-17B-16E-Instruct"
  # - repo_id: "ai21labs/AI21-Jamba-Mini-1.6"
  # quants
  # dyamic / unsloth
  # - repo_id: "unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF"
  # bartowski gguf
  # - repo_id: "bartowski/deepcogito_cogito-v1-preview-qwen-32B-GGUF"
  # - repo_id: "bartowski/bartowski/nvidia_Llama-3_3-Nemotron-Super-49B-v1-GGUF"
  # - repo_id: "bartowski/Skywork_Skywork-OR1-32B-Preview-GGUF"
  # - repo_id: "bartowski/deepcogito_cogito-v1-preview-llama-70B-GGUF"
    # revision: "main" # Optional: specify a branch, tag, or commit hash

# --- vLLM Settings (Docker Container) ---
vllm_docker_image: "vllm/vllm-openai:latest" # Official image. Pin version (e.g., :v0.4.0.post1) for stability
vllm_port: 8000 # Port EXPOSED on the HOST
vllm_model: "mistralai/Mistral-7B-Instruct-v0.1"  # HF model ID
vllm_kv_cache_dtype: "fp8"  # or "auto", "fp16", etc.
vllm_load_format: "auto"
vllm_calculate_kv_scales: true
# vllm_quantization: "awq"  # Uncomment if using quantized models
# Optional vLLM runtime arguments (passed to the container command)
# vllm_tensor_parallel_size: 1 # Number of GPUs for tensor parallelism
# vllm_gpu_memory_utilization: 0.90 # Fraction of GPU memory to use
# vllm_max_model_len: 4096 # Max sequence length

# --- Infinity Settings ---
# Check Docker Hub for appropriate image tags (e.g., specific cuda versions)
infinity_image: "michaelf34/infinity:latest"
infinity_port: 7997 # Port EXPOSED on the HOST
# REPO_ID of the embedding model Infinity should serve (must be in 'models_to_download')
infinity_active_model_repo_id: "sentence-transformers/all-MiniLM-L6-v2"
# Automatically derive the HOST path where the model is downloaded
infinity_model_path_host: "{{ models_base_dir }}/{{ infinity_active_model_repo_id | regex_replace('^.*/', '') }}"
# Define the path INSIDE the container where the model will be mounted
infinity_model_path_container: "/models/{{ infinity_active_model_repo_id | regex_replace('^.*/', '') }}"


# --- Service Control ---
start_services: false  # Set to true to automatically start ollama/infinity/vllm

# --- Ollama Settings ---
# Ollama usually runs on port 11434 and manages its own models/service after installation.

# --- Docker Settings ---
# Docker Compose V2 (plugin) is usually installed via apt now.
